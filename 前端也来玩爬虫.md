# 简介

## 什么是爬虫？

> 百科：网络爬虫，是一种自动获取网页内容的程序。是搜索引擎的重要组成部分，因此搜索引擎优化很大程度上就是针对爬虫而做出的优化。

简单的说，爬虫就是通过程序代码，获取一些URL对应的网页的HTML内容，进行分析，从而发现更多网页，并解析网页，拿到自己需要的数据，进行保存，和进一步利用。

## 有什么用？

### 1. 搜索引擎
> 搜索引擎派出一个能够在网上发现新网页并抓文件的程序，这个程序通常称之为蜘蛛（Spider）。搜索引擎从已知的数据库出发，就像正常用户的浏览器一样访问这些网页并抓取文件。搜索引擎通过这些爬虫去爬互联网上的外链，从这个网站爬到另一个网站，去跟踪网页中的链接，访问更多的网页，这个过程就叫爬行。这些新的网址会被存入数据库等待搜索。

### 2. 其他
> 某电影评分网站，没提供按评分高低排序功能？自己抓下来，自己排。

> 某网站关注的女神，想分析一下女神的过往动态，达到一些不可描述的目的？抓动态，做图表。

> 某旅行服务网站经常爆出酒店机票bug价，却发现不及时，错过几个亿？抓抓抓，监控！

> 某电影资源网站，广告多，打开找电影太麻烦？直接抓下载链接。

> 公司分析竞争对手网站？二话不说，就是爬。

所以说，爬虫也是让你掌握的技术，帮你解决工作之外的问题的一个很好的工具。

## 一般流程

- 抓：把想要的页面抓回来先；
- 存：一般，会把抓回来的内容，先存下来再分析；
- 解：解析抓回来的内容，提取有用的东西；
- 示：把有用的东西，按需求，处理成直观的内容，进行输出；

# 开始：用NodeJS做一个简单的爬虫

然后，我们需要一个idea，做个什么爬虫呢？想了半天，最终决定去爬一下豆瓣读书（其中的一个分类），看看计算机相关的书籍有多少，并分析一下，跟前端相关的有多少，评分如何，价格如何等等。

那么，把大象装冰箱统共需要几步？

1. 需要一个URL。

于是打开豆瓣网站，定位到计算机相关书籍页面，URL如下：

[https://read.douban.com/kind/105?sort=hot&promotion_only=False&min_price=None&works_type=None&max_price=None](https://read.douban.com/kind/105?sort=hot&promotion_only=False&min_price=None&works_type=None&max_price=None)

有了URL，接下来要用代码抓页面了，如何做呢？

2. 在Node环境里解析URL

> 简单起见，本文就不说抓到的数据怎么存储了，存文件，存数据库，或者别的什么方式，大家都可以自由选择，这部分不是本文重点。所以，我们启动一个server，直接把抓到的东西进行分析展示。

这样的话，我们需要`server`：

新建一个文件夹，取名：`read.douban.com`

新建`js`文件：

```javascript
// server.js
const http = require('http')
const start = () => {
    let onRequest = async (req, res) => {
        res.writeHead(200, {
            'Content-Type': 'text/html; charset=UTF-8',
        })
        res.write('' || 'ERR')
        res.end()
    }    
    http.createServer(onRequest).listen(3000)
    console.log('server started!')
}

module.exports = {start}
```
```javascript
// index.js
const server = require('./server')
server.start()
```
代码很简单，就不过多解释了。

到这里，我们已经有了一个基本的node环境服务代码，但是好像跟爬虫，并没有什么关系。

那么，接下来重头戏就要来了：

```javascript
//爬虫的主要业务逻辑模块
const phantom = require('phantom')
const cheerio = require('cheerio')

const userAgent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'
// const urlStr = 'https://read.douban.com/kind/105?start=0&sort=hot'

/**
 * 从每一页的html代码中，提取需要的数据，拼装成json
 * 需要分析页面结构，拿到所需数据的包裹元素
 * @param {string} htmlText 
 * @param {string} pageNum 
 */
function html2json(htmlText, pageNum) {
    const $ = cheerio.load(htmlText)
    let booksList = []

    return new Promise((resolve, reject) => {
        $('.ebook-list').find('.store-item').each((index, ele) => {
            booksList.push({
                order: `${pageNum}_${index + 1}`,
                name: $(ele).find('.title > a').text(),
                price: $(ele).find('.discount-price').text() || $(ele).find('.price-tag').text(),
                author: $(ele).find('.author-item').text(),
                rating: $(ele).find('.rating-average').text() || '0',
                rateCount: $(ele).find('.ratings-link > span').text(),
                desc: $(ele).find('.article-desc-brief').text(),
            })
        })
        resolve(booksList)
    })
}

/**
 * 主要抓取逻辑函数
 * @param {string} maxPage
 */
async function getPageData(maxPage) {
    // 定义一个存储返回结果的数组
    let rstArr = []

    // 创建一个phantom实例，传入一些创建参数
    const instance = await phantom.create([
        '--ignore-ssl-errors=yes',
        '--load-images=no',
        '--web-security=false',
    ])

    // 用phantom实例，创建一个页面实例
    const page = await instance.createPage()
    // 设置浏览器页面的宽高
    await page.property('viewportSize', { width: 1024, height: 768 })
    // 设置浏览器的UA，可以防范一些反爬虫措施
    await page.property('settings', { 
        userAgent: userAgent,
    })
    
    // 根据传入的最大页数，循环抓取数据
    for(let i = 1; i <= maxPage; i++){
        // 生成抓取页面的url
        let url = `https://read.douban.com/kind/105?start=${(i - 1) * 20}&sort=hot`
        
        // 打开页面
        const status = await page.open(url)
        // 获取页面的html内容
        const content = await page.property('content')
        // 用cheerio加载页面内容，方便使用jq语法获取各种元素
        const $ = cheerio.load(content)
        
        // 获取页面内容转换成的，由所需信息转换成的对象数组
        const currentArr = await html2json($('article.col').html(), i).catch(e => {
            console.log(e.toString())
        })
        // 循环结束时，把结果拼到一起
        rstArr = rstArr.concat(currentArr)
    }

    // 关闭phantom实例
    await instance.exit()
    
    // 返回一个promise异步对象
    return new Promise((resolve, reject) => {
        // 处理一些自定义数据逻辑

        // 按评分排序
        rstArr.sort((a, b) => {
            if(a.rating < b.rating){
                return -1
            }
            if(a.rating > b.rating){
                return 1
            }
            return 0
        })

        // 过滤出几个前端关键词
        // rstArr = rstArr.filter(item => {
        //     return item.name.indexOf('js') > -1 || item.name.indexOf('css') > -1 || item.name.indexOf('html') > -1
        // })

        resolve(JSON.stringify(rstArr))
    })
}

/**
 * 出口函数
 */
async function getSpiderResult() {
    // 设定最大抓取页数，调用getPageData，返回结果
    return await getPageData(2).catch(e => {
        console.log(e.toString())
    })
}

module.exports = getSpiderResult()
```
还是方便起见，以上只是用最简单粗暴的方式，实现了爬虫逻辑，以便于理解。

几个依赖工具的说明：

- phantom
    
    由于一些网站的反爬虫措施，我们直接使用这个headless的浏览器引擎，利用它，可以直接使用代码，模拟出真实用户对一个网页的访问，只不过全都是代码来操作，比较抽象。就好像命令行之于操作系统GUI。[官网API](http://phantomjs.org/api/)

- phantomjs - node

    可以在node端使用phantom的工具。[官方Github](https://github.com/amir20/phantomjs-node)

- cheerio

    由于node端并没有浏览器环境，如果想操作一些抓取回来的html元素，就变得很麻烦。cheerio是一个可以让你在node端，使用jq语法来操作html元素的工具。[官方Github](https://github.com/cheeriojs/cheerio)

补充说明：

本文抓取数据时，省略了延时抓取的策略（每次抓取，间隔一定时间），对于一个爬虫来说，这是不安全，不科学，甚至不道德的。因为，如果我们要抓取100页甚至更多数据，每一页抓取都不设延时，那么对于被抓取的网站来说，就是一个短时间的高并发访问，而这个访问，并不真实，甚至类似一种攻击行为。所以很多网站会针对性识别这种访问，并做出限制。

另外，对于一个爬虫，其实存储这一步，还是很重要的，因为抓回来的数据，通常量都比较大，结构也比较复杂，如果直接在代码中进行分析，那么每次运行爬虫，都需要重新抓取一遍数据，这不仅耗时，还可能被网站的安全策略限制，而且没什么必要。

一般的做法，都会在抓到数据之后，分析提取，存储到数据库，并每隔一段时间，更新一遍数据。分析和展示的工作，基于数据库的数据来做，就可以了。

本文只是出于简单易于理解的目的，属于新手向文章，故省略以上步骤。

以上。